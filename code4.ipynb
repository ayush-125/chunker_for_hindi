{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-CCP', 2: 'I-CCP', 3: 'B-NEGP', 4: 'I-NEGP', 5: 'B-JJP', 6: 'I-JJP', 7: 'B-BLK', 8: 'I-BLK', 9: 'B-VGNF', 10: 'I-VGNF', 11: 'B-FRAGP', 12: 'I-FRAGP', 13: 'B-NP', 14: 'I-NP', 15: 'B-VGNN', 16: 'I-VGNN', 17: 'B-VGF', 18: 'I-VGF', 19: 'B-RBP', 20: 'I-RBP'}\n",
      "{0: 'PROPN', 1: 'ADP', 2: 'ADJ', 3: 'PUNCT', 4: 'PART', 5: 'X', 6: 'SCONJ', 7: 'AUX', 8: 'CCONJ', 9: 'DET', 10: 'ADV', 11: 'NOUN', 12: 'VERB', 13: 'NUM', 14: 'INTJ', 15: 'PRON'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayush\\OneDrive\\Documents\\nlp_project\\nlp_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from conllu import parse_incr\n",
    "\n",
    "def load_preprocessed_conllu(file_path):\n",
    "    \"\"\"\n",
    "    Load a preprocessed CoNLL-U formatted file and return a list of sentences with annotations.\n",
    "    Each sentence is represented as a dictionary.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    all_chunk_labels = set()  \n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for tokenlist in parse_incr(f):\n",
    "            sentence = {\n",
    "                'id': tokenlist.metadata.get('sent_id', ''),\n",
    "                'text': tokenlist.metadata.get('text', ''),\n",
    "                'tokens': [token['form'] for token in tokenlist],\n",
    "                'upos': [token['upos'] for token in tokenlist],\n",
    "                'chunk_id': [],\n",
    "                'chunk_type': [],\n",
    "            }\n",
    "\n",
    "            for token in tokenlist:\n",
    "                # Get the 'misc' field, which contains the ChunkId and ChunkType\n",
    "                misc = token.get('misc', '')  # safely access 'misc' field\n",
    "                chunk_id = '_'\n",
    "                chunk_type = 'O'  # 'O' typically means 'outside a chunk'\n",
    "\n",
    "                # Handle 'misc' as a string or dictionary\n",
    "                if isinstance(misc, str):\n",
    "                    # Extract ChunkId and ChunkType from the 'misc' field (if it's a string)\n",
    "                    for item in misc.split('|'):\n",
    "                        if item.startswith('ChunkId='):\n",
    "                            chunk_id = item.split('=')[1]\n",
    "                        elif item.startswith('ChunkType='):\n",
    "                            chunk_type = item.split('=')[1]\n",
    "                elif isinstance(misc, dict):\n",
    "                    # If 'misc' is a dictionary, look for chunk info inside it\n",
    "                    chunk_id = misc.get('ChunkId', '_')\n",
    "                    chunk_type = misc.get('ChunkType', 'O')\n",
    "\n",
    "                # Append chunk data\n",
    "                sentence['chunk_id'].append(chunk_id)\n",
    "                sentence['chunk_type'].append(chunk_type)\n",
    "\n",
    "                # Collect all unique chunk labels (using only chunk_id)\n",
    "                all_chunk_labels.add(chunk_id)\n",
    "\n",
    "            sentences.append(sentence)\n",
    "    return sentences,list(all_chunk_labels)\n",
    "\n",
    "# Example usage:\n",
    "data_dir = \"./data/hi_hdtb/\"\n",
    "preprocessed_dir = \"./data/hi_hdtb_preprocessed_new/\"\n",
    "\n",
    "# Load sentences with chunk info\n",
    "file_path = os.path.join(preprocessed_dir, \"hi_hdtb-ud-train.conllu\")\n",
    "train_sentences,all_chunk_labels = load_preprocessed_conllu(file_path)\n",
    "\n",
    "file_path = os.path.join(preprocessed_dir, \"hi_hdtb-ud-dev.conllu\")\n",
    "eval_sentences,all_chunk_labels2 = load_preprocessed_conllu(file_path)\n",
    "\n",
    "# print(train_sentences[:2])  # print first two sentences for checking\n",
    "all_chunk_labels+=all_chunk_labels2\n",
    "\n",
    "file_path = os.path.join(preprocessed_dir, \"hi_hdtb-ud-test.conllu\")\n",
    "test_sentences,all_chunk_labels3=load_preprocessed_conllu(file_path)\n",
    "\n",
    "# print(\"Unique chunk labels:\", all_chunk_labels)\n",
    "def get_chunk_type_from_id(chunk_id):   #not used\n",
    "    return ''.join([c for c in chunk_id if not c.isdigit()])\n",
    "# if not c.isdigit()\n",
    "\n",
    "all_chunk_labelschunk_labels = [get_chunk_type_from_id(chunk) for chunk in all_chunk_labels]\n",
    "all_chunk_labels=list(set(all_chunk_labelschunk_labels))\n",
    "# print(all_chunk_labels)\n",
    "\n",
    "\n",
    "# train_sentences=train_sentences[:100]\n",
    "# eval_sentences=eval_sentences[:10]\n",
    "\n",
    "# print(len(train_sentences))\n",
    "# print(len(eval_sentences))\n",
    "\n",
    "# Encode the UPOS tags\n",
    "upos_tags_set = set()\n",
    "# upos_tags_set.add('O')\n",
    "for sentence in train_sentences + eval_sentences:\n",
    "    upos_tags_set.update(sentence['upos'])\n",
    "upos_tag2id = {tag: idx for idx, tag in enumerate(upos_tags_set)}\n",
    "id2upos = {v: k for k, v in upos_tag2id.items()}\n",
    "\n",
    "# Define BIO tags\n",
    "chunk_tags = {\"O\": 0}  # Outside tag\n",
    "for label in all_chunk_labels:\n",
    "    chunk_type = get_chunk_type_from_id(label)\n",
    "    if f\"B-{chunk_type}\" not in chunk_tags:\n",
    "        chunk_tags[f\"B-{label}\"] = len(chunk_tags)\n",
    "        chunk_tags[f\"I-{label}\"] = len(chunk_tags)\n",
    "\n",
    "id2tag = {v: k for k, v in chunk_tags.items()}\n",
    "\n",
    "print(id2tag)\n",
    "print(id2upos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"xlm-roberta-base\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define Dataset\n",
    "class ChunkingDataset(Dataset):\n",
    "    def __init__(self, sentences, chunk_tags, tokenizer, upos_tag2id, max_len=128):\n",
    "        self.sentences = sentences\n",
    "        self.chunk_tags = chunk_tags\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.upos_tag2id = upos_tag2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tokens = sentence[\"tokens\"]\n",
    "        chunk_ids = sentence[\"chunk_id\"]\n",
    "        upos_tags = [self.upos_tag2id.get(tag, -1) for tag in sentence[\"upos\"]]\n",
    "\n",
    "        if not tokens:\n",
    "            raise ValueError(f\"No tokens found for sentence at index {idx}.\")\n",
    "\n",
    "        # Tokenize inputs\n",
    "        encoded = self.tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        word_ids = encoded.word_ids()\n",
    "        labels, aligned_upos_tags = [], []\n",
    "\n",
    "        prev_chunk_id = None\n",
    "        for i, word_id in enumerate(word_ids):\n",
    "            if word_id is None:\n",
    "                labels.append(-100)  # Ignored in loss computation\n",
    "                aligned_upos_tags.append(-1)\n",
    "            else:\n",
    "                current_chunk_id = chunk_ids[word_id]\n",
    "                upos_tag = upos_tags[word_id]\n",
    "                if current_chunk_id != prev_chunk_id:\n",
    "                    label = f\"B-{get_chunk_type_from_id(current_chunk_id)}\"\n",
    "                else:\n",
    "                    label = f\"I-{get_chunk_type_from_id(current_chunk_id)}\"\n",
    "\n",
    "                labels.append(self.chunk_tags.get(label, 0))  # Default to \"O\"\n",
    "                aligned_upos_tags.append(upos_tag)\n",
    "                prev_chunk_id = current_chunk_id\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"upos_tags\": torch.tensor(aligned_upos_tags, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = ChunkingDataset(train_sentences, chunk_tags, tokenizer, upos_tag2id)\n",
    "eval_dataset = ChunkingDataset(eval_sentences, chunk_tags, tokenizer, upos_tag2id)\n",
    "test_dataset = ChunkingDataset(test_sentences,chunk_tags,tokenizer,upos_tag2id)\n",
    "num_labels = len(chunk_tags)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoModelForTokenClassification, XLMRobertaForSequenceClassification, AutoConfig\n",
    "\n",
    "\n",
    "\n",
    "class CustomModel(XLMRobertaForSequenceClassification):\n",
    "    def __init__(self, base_model_name, num_labels, num_upos_tags, upos_padding_idx=-1):\n",
    "        config = AutoConfig.from_pretrained(base_model_name)\n",
    "        super(CustomModel, self).__init__(config)\n",
    "        hidden_size = self.config.hidden_size\n",
    "        self.upos_embedding = nn.Embedding(num_upos_tags, hidden_size, padding_idx=upos_padding_idx)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(hidden_size * 2, num_labels)\n",
    "        self.num_labels=num_labels\n",
    "        self.gradient_checkpointing_enable()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, upos_tags, labels=None):\n",
    "        # Replace -1 with the padding index\n",
    "        upos_tags = torch.where(upos_tags == -1, torch.tensor(self.upos_embedding.padding_idx).to(upos_tags.device), upos_tags)\n",
    "        \n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "\n",
    "        # Check if there are any invalid values in upos_tags\n",
    "        if torch.any(upos_tags >= self.upos_embedding.num_embeddings):\n",
    "            invalid_indices = upos_tags[upos_tags >= self.upos_embedding.num_embeddings]\n",
    "            print(f\"Invalid upos_tags indices: {invalid_indices}\")\n",
    "            raise ValueError(\"Some upos_tags indices are out of bounds.\")\n",
    "\n",
    "        # Print upos_tags for debugging\n",
    "        # print(f\"upos_tags: {upos_tags}\")\n",
    "\n",
    "        upos_embeds = self.upos_embedding(upos_tags)\n",
    "        # print(f\"Shape of sequence_output: {sequence_output.shape}\")\n",
    "        # print(f\"Shape of upos_embeds: {upos_embeds.shape}\")\n",
    "\n",
    "        if sequence_output.size(1) != upos_embeds.size(1):\n",
    "            raise ValueError(\n",
    "                f\"Mismatch in sequence length: sequence_output ({sequence_output.size(1)}) \"\n",
    "                f\"and upos_embeds ({upos_embeds.size(1)})\"\n",
    "            )\n",
    "\n",
    "        combined_features = torch.cat((sequence_output, upos_embeds), dim=-1)\n",
    "        logits = self.classifier(self.dropout(combined_features))\n",
    "        \n",
    "        # Reshape logits and labels to match [batch_size * seq_len, num_labels] \n",
    "        batch_size, seq_len, _ = sequence_output.shape\n",
    "        # Ensure logits and labels have the same shape\n",
    "        logits = logits.view(batch_size * seq_len, self.num_labels)\n",
    "        if labels is not None:\n",
    "            labels = labels.view(batch_size*seq_len)\n",
    "        \n",
    "        # Print shapes for debugging\n",
    "        # print(f\"Logits shape: {logits.shape}\") \n",
    "        # print(f\"Labels shape: {labels.shape}\")\n",
    "        # Print shapes for debugging \n",
    "        # print(f\"Logits shape: {logits.shape}\") \n",
    "        # print(f\"Labels shape: {labels.shape if labels is not None else 'None'}\")\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer, TrainingArguments\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # class CustomTrainer(Trainer):\n",
    "# #     def prediction_step(self, model, inputs, prediction_loss_only=False, ignore_keys=None):\n",
    "# #         has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
    "# #         inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "# #         with torch.no_grad():\n",
    "# #             outputs = model(**inputs)\n",
    "# #             logits = outputs.get(\"logits\") if isinstance(outputs, dict) else outputs[1]\n",
    "\n",
    "# #         if has_labels:\n",
    "# #             labels = inputs.get(\"labels\")\n",
    "# #         else:\n",
    "# #             labels = None\n",
    "\n",
    "# #         return (logits, labels)\n",
    "    \n",
    "# #     # def evaluation_loop(self, dataloader, description, prediction_loss_only=False, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "# #     #     model = self.model\n",
    "# #     #     model.eval()\n",
    "\n",
    "# #     #     all_metrics = []\n",
    "\n",
    "# #     #     for step, inputs in enumerate(dataloader):\n",
    "# #     #         outputs = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
    "# #     #         logits, labels = outputs\n",
    "\n",
    "# #     #         logits = logits.cpu().numpy()\n",
    "# #     #         labels = labels.cpu().numpy()\n",
    "\n",
    "# #     #         # Ensure logits are reshaped to match the dimensions of labels\n",
    "# #     #         batch_size, seq_len = labels.shape[:2]\n",
    "# #     #         num_labels = logits.shape[-1]\n",
    "# #     #         logits = logits.reshape((batch_size, seq_len, num_labels))\n",
    "\n",
    "# #     #         predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "# #     #         batch_metrics = self.compute_metrics((predictions, labels))\n",
    "# #     #         all_metrics.append(batch_metrics)\n",
    "\n",
    "# #     #     # Aggregate metrics across batches\n",
    "# #     #     avg_metrics = {\n",
    "# #     #         key: np.mean([metric[key] for metric in all_metrics]) for key in all_metrics[0]\n",
    "# #     #     }\n",
    "# #     #     return { \"metrics\": avg_metrics, }\n",
    "# #     #     return avg_metrics\n",
    "# #     def evaluation_loop(self, dataloader, description, prediction_loss_only=False, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "# #         model = self.model\n",
    "# #         model.eval()\n",
    "\n",
    "# #         all_metrics = []\n",
    "\n",
    "# #         for step, inputs in enumerate(dataloader):\n",
    "# #             outputs = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
    "# #             logits, labels = outputs\n",
    "\n",
    "# #             logits = logits.cpu().numpy()\n",
    "# #             labels = labels.cpu().numpy()\n",
    "\n",
    "# #             # Ensure logits are reshaped to match the dimensions of labels\n",
    "# #             batch_size, seq_len = labels.shape[:2]\n",
    "# #             num_labels = logits.shape[-1]\n",
    "# #             logits = logits.reshape((batch_size, seq_len, num_labels))\n",
    "\n",
    "# #             predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "# #             batch_metrics = self.compute_metrics((predictions, labels))\n",
    "# #             all_metrics.append(batch_metrics)\n",
    "\n",
    "# #         # Aggregate metrics across batches\n",
    "# #         avg_metrics = {\n",
    "# #             key: np.mean([metric[key] for metric in all_metrics]) for key in all_metrics[0]\n",
    "# #         }\n",
    "\n",
    "# #         # Debugging: print the avg_metrics structure\n",
    "# #         print(\"Avg Metrics:\", avg_metrics)\n",
    "\n",
    "# #         return {\"metrics\": avg_metrics}\n",
    "    \n",
    "# #     def train(self, *args, **kwargs): \n",
    "# #         # Before calling _maybe_log_save_evaluate \n",
    "# #         print(\"Starting training...\") \n",
    "# #         result = super().train(*args, **kwargs) \n",
    "# #         # Debugging: print the output of evaluation loop \n",
    "# #         output = self.evaluation_loop(dataloader, description=\"Evaluation\") \n",
    "# #         print(\"Output before _maybe_log_save_evaluate:\", output) \n",
    "# #         # Check if 'metrics' exists in 'output' \n",
    "# #         if 'metrics' in output: \n",
    "# #             print(\"Metrics found in output:\", output['metrics']) \n",
    "# #         else: \n",
    "# #             print(\"Metrics key not found in output. Output keys are:\", output.keys()) \n",
    "# #             # Call _maybe_log_save_evaluate\n",
    "# #         self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval) \n",
    "# #         return result\n",
    "\n",
    "\n",
    "# from transformers import Trainer, TrainingArguments\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # class CustomTrainer(Trainer):\n",
    "# #     def prediction_step(self, model, inputs, prediction_loss_only=False, ignore_keys=None):\n",
    "# #         has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
    "# #         inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "# #         with torch.no_grad():\n",
    "# #             outputs = model(**inputs)\n",
    "# #             logits = outputs.get(\"logits\") if isinstance(outputs, dict) else outputs[1]\n",
    "\n",
    "# #         if has_labels:\n",
    "# #             labels = inputs.get(\"labels\")\n",
    "# #         else:\n",
    "# #             labels = None\n",
    "\n",
    "# #         return (logits, labels)\n",
    "    \n",
    "# #     def evaluation_loop(self, dataloader, description, prediction_loss_only=False, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "# #         model = self.model\n",
    "# #         model.eval()\n",
    "\n",
    "# #         all_metrics = []\n",
    "\n",
    "# #         for step, inputs in enumerate(dataloader):\n",
    "# #             outputs = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
    "# #             logits, labels = outputs\n",
    "\n",
    "# #             logits = logits.cpu().numpy()\n",
    "# #             labels = labels.cpu().numpy()\n",
    "\n",
    "# #             # Ensure logits are reshaped to match the dimensions of labels\n",
    "# #             batch_size, seq_len = labels.shape[:2]\n",
    "# #             num_labels = logits.shape[-1]\n",
    "# #             logits = logits.reshape((batch_size, seq_len, num_labels))\n",
    "\n",
    "# #             predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "# #             batch_metrics = self.compute_metrics((predictions, labels))\n",
    "# #             all_metrics.append(batch_metrics)\n",
    "\n",
    "# #         # Aggregate metrics across batches\n",
    "# #         avg_metrics = {\n",
    "# #             key: np.mean([metric[key] for metric in all_metrics]) for key in all_metrics[0]\n",
    "# #         }\n",
    "\n",
    "# #         # Debugging: print the avg_metrics structure\n",
    "# #         print(\"Avg Metrics:\", avg_metrics)\n",
    "\n",
    "# #         # Return the output with the metrics key\n",
    "# #         return {\"metrics\": avg_metrics}\n",
    "    \n",
    "# #     def train(self, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None, **kwargs):\n",
    "# #         # Before calling _maybe_log_save_evaluate\n",
    "# #         print(\"Starting training...\")\n",
    "\n",
    "# #         # Create dataloader for evaluation\n",
    "# #         eval_dataloader = self.get_eval_dataloader()\n",
    "\n",
    "# #         # Debugging: print the output of evaluation loop\n",
    "# #         output = self.evaluation_loop(eval_dataloader, description=\"Evaluation\")\n",
    "# #         print(\"Output before _maybe_log_save_evaluate:\", output)\n",
    "\n",
    "# #         # Check if 'metrics' exists in 'output'\n",
    "# #         if 'metrics' in output:\n",
    "# #             print(\"Metrics found in output:\", output['metrics'])\n",
    "# #         else:\n",
    "# #             print(\"Metrics key not found in output. Output keys are:\", output.keys())\n",
    "        \n",
    "# #         # Call the original train method\n",
    "# #         result = super().train(resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n",
    "\n",
    "# #         return result\n",
    "\n",
    "# # from transformers import Trainer, TrainingArguments\n",
    "# # import numpy as np\n",
    "# # from sklearn.metrics import classification_report\n",
    "\n",
    "# # class CustomTrainer(Trainer):\n",
    "# #     def prediction_step(self, model, inputs, prediction_loss_only=False, ignore_keys=None):\n",
    "# #         has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
    "# #         inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "# #         with torch.no_grad():\n",
    "# #             outputs = model(**inputs)\n",
    "# #             logits = outputs.get(\"logits\") if isinstance(outputs, dict) else outputs[1]\n",
    "\n",
    "# #         if has_labels:\n",
    "# #             labels = inputs.get(\"labels\")\n",
    "# #         else:\n",
    "# #             labels = None\n",
    "\n",
    "# #         return (logits, labels)\n",
    "    \n",
    "# #     def evaluation_loop(self, dataloader, description, prediction_loss_only=False, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "# #         model = self.model\n",
    "# #         model.eval()\n",
    "\n",
    "# #         all_metrics = []\n",
    "\n",
    "# #         for step, inputs in enumerate(dataloader):\n",
    "# #             outputs = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
    "# #             logits, labels = outputs\n",
    "\n",
    "# #             logits = logits.cpu().numpy()\n",
    "# #             labels = labels.cpu().numpy()\n",
    "\n",
    "# #             # Ensure logits are reshaped to match the dimensions of labels\n",
    "# #             batch_size, seq_len = labels.shape[:2]\n",
    "# #             num_labels = logits.shape[-1]\n",
    "# #             logits = logits.reshape((batch_size, seq_len, num_labels))\n",
    "\n",
    "# #             predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "# #             batch_metrics = self.compute_metrics((predictions, labels))\n",
    "# #             all_metrics.append(batch_metrics)\n",
    "\n",
    "# #         # Aggregate metrics across batches\n",
    "# #         avg_metrics = {\n",
    "# #             key: np.mean([metric[key] for metric in all_metrics]) for key in all_metrics[0]\n",
    "# #         }\n",
    "\n",
    "# #         # Debugging: print the avg_metrics structure\n",
    "# #         print(\"Avg Metrics:\", avg_metrics)\n",
    "\n",
    "# #         # Return the output with the metrics key\n",
    "# #         return {\"metrics\": avg_metrics}\n",
    "    \n",
    "# #     def train(self, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None, **kwargs):\n",
    "# #         # Before calling _maybe_log_save_evaluate\n",
    "# #         print(\"Starting training...\")\n",
    "\n",
    "# #         # Call the original train method\n",
    "# #         result = super().train(resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n",
    "\n",
    "# #         return result\n",
    "\n",
    "# #     def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval):\n",
    "# #         # Call the original method and add debugging\n",
    "# #         metrics = None\n",
    "# #         if self.control.should_evaluate:\n",
    "# #             # Debugging: print the output of evaluation\n",
    "# #             output = self._evaluate(trial, ignore_keys_for_eval)\n",
    "# #             print(\"Output of _evaluate:\", output)\n",
    "# #             if 'metrics' in output:\n",
    "# #                 print(\"Metrics found in output:\", output['metrics'])\n",
    "# #             else:\n",
    "# #                 print(\"Metrics key not found in output. Output keys are:\", output.keys())\n",
    "# #             metrics = output\n",
    "        \n",
    "# #         super()._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n",
    "\n",
    "# from transformers import Trainer, TrainingArguments\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# class CustomTrainer(Trainer):\n",
    "#     def prediction_step(self, model, inputs, prediction_loss_only=False, ignore_keys=None):\n",
    "#         has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
    "#         inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs)\n",
    "#             logits = outputs.get(\"logits\") if isinstance(outputs, dict) else outputs[1]\n",
    "\n",
    "#         if has_labels:\n",
    "#             labels = inputs.get(\"labels\")\n",
    "#         else:\n",
    "#             labels = None\n",
    "\n",
    "#         return (logits, labels)\n",
    "    \n",
    "#     def evaluation_loop(self, dataloader, description, prediction_loss_only=False, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "#         model = self.model\n",
    "#         model.eval()\n",
    "\n",
    "#         all_metrics = []\n",
    "\n",
    "#         for step, inputs in enumerate(dataloader):\n",
    "#             outputs = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
    "#             logits, labels = outputs\n",
    "\n",
    "#             logits = logits.cpu().numpy()\n",
    "#             labels = labels.cpu().numpy()\n",
    "\n",
    "#             # Ensure logits are reshaped to match the dimensions of labels\n",
    "#             batch_size, seq_len = labels.shape[:2]\n",
    "#             num_labels = logits.shape[-1]\n",
    "#             logits = logits.reshape((batch_size, seq_len, num_labels))\n",
    "\n",
    "#             predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "#             batch_metrics = self.compute_metrics((predictions, labels))\n",
    "#             all_metrics.append(batch_metrics)\n",
    "\n",
    "#         # Aggregate metrics across batches\n",
    "#         avg_metrics = {\n",
    "#             key: np.mean([metric[key] for metric in all_metrics]) for key in all_metrics[0]\n",
    "#         }\n",
    "\n",
    "#         # Debugging: print the avg_metrics structure\n",
    "#         print(\"Avg Metrics:\", avg_metrics)\n",
    "\n",
    "#         # Return the output with the metrics key\n",
    "#         return {\"metrics\": avg_metrics}\n",
    "    \n",
    "#     def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"\"):\n",
    "#         eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "#         output = self.evaluation_loop(eval_dataloader, description=\"Evaluation\", metric_key_prefix=metric_key_prefix)\n",
    "#         metrics = output.get(\"metrics\", {})\n",
    "#         self.log(metrics)\n",
    "#         return metrics\n",
    "    \n",
    "#     def train(self, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None, **kwargs):\n",
    "#         # Before calling _maybe_log_save_evaluate\n",
    "#         print(\"Starting training...\")\n",
    "\n",
    "#         # Call the original train method\n",
    "#         result = super().train(resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n",
    "\n",
    "#         return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only=False, ignore_keys=None):\n",
    "        has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.get(\"logits\") if isinstance(outputs, dict) else outputs[1]\n",
    "\n",
    "        if has_labels:\n",
    "            labels = inputs.get(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        return (logits, labels)\n",
    "    \n",
    "    def evaluation_loop(self, dataloader, description, prediction_loss_only=False, ignore_keys=None, metric_key_prefix=\"\"):\n",
    "        model = self.model\n",
    "        model.eval()\n",
    "\n",
    "        all_metrics = []\n",
    "\n",
    "        for step, inputs in enumerate(dataloader):\n",
    "            outputs = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
    "            logits, labels = outputs\n",
    "\n",
    "            logits = logits.cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "            # Ensure logits are reshaped to match the dimensions of labels\n",
    "            batch_size, seq_len = labels.shape[:2]\n",
    "            num_labels = logits.shape[-1]\n",
    "            logits = logits.reshape((batch_size, seq_len, num_labels))\n",
    "\n",
    "            predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "            batch_metrics = self.compute_metrics((predictions, labels))\n",
    "            all_metrics.append(batch_metrics)\n",
    "\n",
    "        # Aggregate metrics across batches\n",
    "        avg_metrics = {\n",
    "            key: np.mean([metric[key] for metric in all_metrics]) for key in all_metrics[0]\n",
    "        }\n",
    "\n",
    "        # Ensure the evaluation metrics include f1\n",
    "        if 'f1' not in avg_metrics:\n",
    "            avg_metrics['f1'] = 0.0  # Default value if f1 is missing\n",
    "\n",
    "        # Debugging: print the avg_metrics structure\n",
    "        # print(\"Avg Metrics:\", avg_metrics)\n",
    "\n",
    "        # Return the output with the metrics key\n",
    "        return {\"metrics\": avg_metrics}\n",
    "    \n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "        output = self.evaluation_loop(eval_dataloader, description=\"Evaluation\", metric_key_prefix=metric_key_prefix)\n",
    "        metrics = output.get(\"metrics\", {})\n",
    "        self.log(metrics)\n",
    "\n",
    "        # Debugging: print the metrics structure\n",
    "        # print(\"Metrics in evaluate method:\", metrics)\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def train(self, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None, **kwargs):\n",
    "        # Before calling _maybe_log_save_evaluate\n",
    "        # print(\"Starting training...\")\n",
    "\n",
    "        # Call the original train method\n",
    "        result = super().train(resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval):\n",
    "        # print(\"In _maybe_log_save_evaluate\")\n",
    "        \n",
    "        metrics = None\n",
    "        if self.control.should_evaluate:\n",
    "            metrics = self._evaluate(trial, ignore_keys_for_eval)\n",
    "\n",
    "        # Debugging: print the metrics structure\n",
    "        # if metrics:\n",
    "        #     print(\"Metrics in _maybe_log_save_evaluate:\", metrics)\n",
    "\n",
    "        super()._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    # print(f\"Predictions shape: {predictions.shape}\")\n",
    "    # print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "    predictions = predictions.reshape(-1)\n",
    "\n",
    "    # Flatten labels to match predictions\n",
    "    labels = labels.reshape(-1)\n",
    "\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "\n",
    "    for pred, label in zip(predictions, labels):\n",
    "        if label != -100:  # Ensuring ignored tokens are not included\n",
    "            true_labels.append(id2tag[label])\n",
    "            true_predictions.append(id2tag[pred])\n",
    "\n",
    "    report = classification_report(true_labels, true_predictions, output_dict=True, zero_division=0)\n",
    "    precision = report[\"weighted avg\"][\"precision\"]\n",
    "    recall = report[\"weighted avg\"][\"recall\"]\n",
    "    f1 = report[\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "    return {\n",
    "        \"eval_precision\": precision,\n",
    "        \"eval_recall\": recall,\n",
    "        \"eval_f1\": f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayush\\OneDrive\\Documents\\nlp_project\\nlp_venv\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_6360\\2477123491.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      " 20%|â–ˆâ–ˆ        | 208/1040 [46:26<2:53:55, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_precision': np.float64(0.8514097154465643), 'eval_recall': np.float64(0.8437951472554641), 'eval_f1': np.float64(0.8429851381053163), 'f1': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 208/1040 [46:46<2:53:55, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_precision': np.float64(0.8514097154465643), 'eval_recall': np.float64(0.8437951472554641), 'eval_f1': np.float64(0.8429851381053163), 'f1': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 416/1040 [1:33:19<2:13:26, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_precision': np.float64(0.8763609499339371), 'eval_recall': np.float64(0.8755407762551842), 'eval_f1': np.float64(0.8728581597678207), 'f1': 0.0, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 416/1040 [1:33:38<2:13:26, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_precision': np.float64(0.8763609499339371), 'eval_recall': np.float64(0.8755407762551842), 'eval_f1': np.float64(0.8728581597678207), 'f1': 0.0, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 500/1040 [1:52:14<1:56:58, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4623, 'grad_norm': 3.6304471492767334, 'learning_rate': 2.5961538461538464e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 624/1040 [2:19:48<1:29:06, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_precision': np.float64(0.9135819645990464), 'eval_recall': np.float64(0.9123480860494856), 'eval_f1': np.float64(0.9106155267487667), 'f1': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 624/1040 [2:20:14<1:29:06, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_precision': np.float64(0.9135819645990464), 'eval_recall': np.float64(0.9123480860494856), 'eval_f1': np.float64(0.9106155267487667), 'f1': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 832/1040 [3:08:35<45:34, 13.15s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_precision': np.float64(0.9302788926518324), 'eval_recall': np.float64(0.9293709378735152), 'eval_f1': np.float64(0.9282097573923902), 'f1': 0.0, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 832/1040 [3:08:54<45:34, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_precision': np.float64(0.9302788926518324), 'eval_recall': np.float64(0.9293709378735152), 'eval_f1': np.float64(0.9282097573923902), 'f1': 0.0, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1000/1040 [9:14:19<08:39, 12.99s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1969, 'grad_norm': 4.482397556304932, 'learning_rate': 1.9230769230769234e-06, 'epoch': 4.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1040/1040 [9:23:38<00:00, 12.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_precision': np.float64(0.93665803502294), 'eval_recall': np.float64(0.936585924637009), 'eval_f1': np.float64(0.9350812109270854), 'f1': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1040/1040 [9:24:03<00:00, 12.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_precision': np.float64(0.93665803502294), 'eval_recall': np.float64(0.936585924637009), 'eval_f1': np.float64(0.9350812109270854), 'f1': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1040/1040 [9:24:12<00:00, 32.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 33852.9192, 'train_samples_per_second': 1.965, 'train_steps_per_second': 0.031, 'train_loss': 0.32278266824208773, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1040, training_loss=0.32278266824208773, metrics={'train_runtime': 33852.9192, 'train_samples_per_second': 1.965, 'train_steps_per_second': 0.031, 'total_flos': 4347588916846080.0, 'train_loss': 0.32278266824208773, 'epoch': 5.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Data and Model\n",
    "model = CustomModel(\n",
    "    base_model_name=model_name,\n",
    "    num_labels=num_labels,\n",
    "    num_upos_tags=len(upos_tag2id),\n",
    "    upos_padding_idx=-1\n",
    ")\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    no_cuda=False,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    gradient_accumulation_steps=4\n",
    ")\n",
    "\n",
    "# Create Data Collator\n",
    "class CustomDataCollator:\n",
    "    def __call__(self, features):\n",
    "        input_ids = torch.stack([f[\"input_ids\"] for f in features])\n",
    "        attention_mask = torch.stack([f[\"attention_mask\"] for f in features])\n",
    "        upos_tags = torch.stack([f[\"upos_tags\"] for f in features])\n",
    "        labels = torch.stack([f[\"labels\"] for f in features])\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"upos_tags\": upos_tags,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "data_collator = CustomDataCollator()\n",
    "\n",
    "# Initialize Custom Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "# Training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to chunker-fine-tuned-xlm-roberta-hindi-3\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the model\n",
    "model_save_path = \"chunker-fine-tuned-xlm-roberta-hindi-3\"\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: {'eval_precision': np.float64(0.93665803502294), 'eval_recall': np.float64(0.936585924637009), 'eval_f1': np.float64(0.9350812109270854), 'f1': 0.0, 'epoch': 5.0}\n",
      "Test Metrics: {'eval_precision': np.float64(0.9412401670025333), 'eval_recall': np.float64(0.9410789094038007), 'eval_f1': np.float64(0.9396890267530904), 'f1': 0.0, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the validation set\n",
    "val_metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "print(\"Validation Metrics:\", val_metrics)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(\"Test Metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('à¤­à¤¾à¤°à¤¤', 'NP'), ('à¤à¤•', 'NP'), ('à¤¸à¥à¤‚à¤¦à¤° à¤¦à¥‡à¤¶ à¤¹à¥ˆ', 'NP')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example Inference Function for POS and Chunking\n",
    "def predict_chunking_with_pos(sentence, tokenizer, model, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the sentence with POS tags as input\n",
    "    tokens = sentence['tokens']\n",
    "    pos_tags = sentence['upos']\n",
    "\n",
    "    # Tokenize using the tokenizer\n",
    "    inputs = tokenizer(tokens, is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Align the POS tags with tokenized tokens\n",
    "    aligned_pos_tags = []\n",
    "    original_to_tok_map = []  # Maps each original token to its first subtoken\n",
    "    for i, (word, pos) in enumerate(zip(tokens, pos_tags)):\n",
    "        subword_tokens = tokenizer.tokenize(word)\n",
    "        original_to_tok_map.append(len(aligned_pos_tags))\n",
    "        aligned_pos_tags.extend([upos_tag2id.get(pos, -1)] * len(subword_tokens))\n",
    "\n",
    "    # Ensure POS tags are aligned with tokenized words, and pad if necessary\n",
    "    while len(aligned_pos_tags) < inputs['input_ids'].size(1):\n",
    "        aligned_pos_tags.append(-1)  # Padding value for unknown or extra tokens\n",
    "    aligned_pos_tags = aligned_pos_tags[:inputs['input_ids'].size(1)]  # Ensure no extra padding\n",
    "\n",
    "    # Convert aligned POS tags to tensor and add to inputs\n",
    "    inputs['upos_tags'] = torch.tensor([aligned_pos_tags]).to(device)\n",
    "\n",
    "    # Perform inference (no gradient computation)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], upos_tags=inputs['upos_tags'])\n",
    "        logits = outputs[1] if isinstance(outputs, tuple) else outputs  # Ensure you get logits tensor\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Decode predictions (convert label IDs to tags)\n",
    "    predicted_ids = predictions.squeeze().cpu().numpy()\n",
    "    predicted_labels = [id2tag[label_id] for label_id in predicted_ids]\n",
    "\n",
    "    # Map predicted labels back to original tokens\n",
    "    original_predictions = [predicted_labels[original_to_tok_map[i]] for i in range(len(tokens))]\n",
    "\n",
    "    return original_predictions\n",
    "\n",
    "# Function to convert BIO tags to chunked phrases with types\n",
    "def bio_to_chunks(tokens, bio_tags):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_label = None\n",
    "\n",
    "    for token, tag in zip(tokens, bio_tags):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if current_chunk:\n",
    "                chunks.append((' '.join(current_chunk), current_label))\n",
    "                current_chunk = []\n",
    "            current_label = tag[2:]\n",
    "            current_chunk.append(token)\n",
    "        elif tag.startswith(\"I-\") and current_label == tag[2:]:\n",
    "            current_chunk.append(token)\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append((' '.join(current_chunk), current_label))\n",
    "            current_chunk = []\n",
    "            if tag != \"O\":\n",
    "                current_label = tag[2:]\n",
    "                current_chunk.append(token)\n",
    "            else:\n",
    "                current_label = None\n",
    "                current_chunk.append(token)  # Capture \"O\" tokens as separate chunks\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append((' '.join(current_chunk), current_label))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Test sentence with POS tagging\n",
    "test_sentence = {\n",
    "    \"tokens\": [\"à¤­à¤¾à¤°à¤¤\", \"à¤à¤•\", \"à¤¸à¥à¤‚à¤¦à¤°\", \"à¤¦à¥‡à¤¶\", \"à¤¹à¥ˆ\"],\n",
    "    \"upos\": [\"PROPN\", \"DET\", \"ADJ\", \"NOUN\", \"AUX\"]  # POS tags for Hindi sentence\n",
    "}\n",
    "\n",
    "# Make sure your model and tokenizer are loaded before calling this function\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "predicted_chunking = predict_chunking_with_pos(test_sentence, tokenizer, model, device)\n",
    "# print(predicted_chunking)\n",
    "chunks = bio_to_chunks(test_sentence['tokens'], predicted_chunking)\n",
    "print(chunks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
