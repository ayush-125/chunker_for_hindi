{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report: Hindi Chunking using XLM-RoBERTa for Token Classification\n",
    "\n",
    "## 1. Introduction\n",
    "The objective of this project is to develop a chunking model for Hindi language using a pre-trained multilingual model, **XLM-RoBERTa**, for token classification. Chunking is the process of grouping individual tokens (words) into syntactic units or phrases, such as noun phrases (NP), verb phrases (VP), or prepositional phrases (PP). This project aims to preprocess the dataset, create chunk labels (BIO format), and fine-tune a transformer-based model for accurate chunk predictions.\n",
    "\n",
    "In this report, we describe the dataset preparation, model architecture, experiments, and evaluation results for chunking Hindi text. We utilized the Universal Dependencies (UD) Hindi Treebank and categorized tokens based on their Universal POS tags into six categories: **NP**, **VP**, **ADJP**, **ADVP**, **PP**, and **Other**.\n",
    "\n",
    "## 2. Dataset and Preprocessing\n",
    "The dataset used for this task is the **Hindi UD Treebank (HDTB)**, which provides annotated text in the CoNLL-U format. We preprocessed the dataset to remove extra columns, normalize the sentence structure, and generate chunk labels. The data is split into training, development, and test sets:\n",
    "<!-- - **Training Set**: 13,304 sentences\n",
    "- **Development Set**: 1,657 sentences\n",
    "- **Test Set**: 1,661 sentences -->\n",
    "\n",
    "The preprocessing steps involved:\n",
    "1. Trimming each line to retain the first 10 columns of CoNLL-U files.\n",
    "2. Loading sentences and extracting tokens, Universal POS tags (UPOS), and dependency relations.\n",
    "3. Assigning chunk labels (BIO format) based on UPOS tags.\n",
    "\n",
    "Chunk labels were generated by categorizing tokens as follows:\n",
    "- **NP**: For tokens labeled as PROPN, NOUN, or PRON.\n",
    "- **VP**: For tokens labeled as VERB.\n",
    "- **ADJP**: For tokens labeled as ADJ.\n",
    "- **ADVP**: For tokens labeled as ADV.\n",
    "- **PP**: For tokens labeled as ADP.\n",
    "- **Other**: For all other tokens.\n",
    "\n",
    "### Data Representation:\n",
    "Each sentence was represented with:\n",
    "- **Tokens**: Words or subwords in the sentence.\n",
    "- **UPOS**: Universal part-of-speech tags for each token.\n",
    "- **Chunk Labels**: BIO (Beginning-Inside-Outside) chunk labels for syntactic chunks.\n",
    "\n",
    "## 3. Model and Experimental Setup\n",
    "For this task, we employed **XLM-RoBERTa**, a transformer-based multilingual model, fine-tuned for token classification using chunk labels. The model was initialized with pre-trained weights from the **xlm-roberta-base** model and fine-tuned on the labeled Hindi chunking data.The model was fine-tuned with gradient checkpointing enabled to optimize memory usage.\n",
    "\n",
    "### Training Configuration:\n",
    "- **Batch Size**: 2 (due to memory constraints)\n",
    "- **Learning Rate**: 3e-5\n",
    "- **Epochs**: 5\n",
    "- **Evaluation Strategy**: Evaluated on the development set after each epoch using precision, recall, and F1-score.\n",
    "\n",
    "### Training Setup\n",
    "The model was trained for 5 epochs with a learning rate of 3e-5, a batch size of 2, and weight decay of 0.01. Gradient accumulation was set to 4 to improve learning despite small batch sizes. The compute_metrics() function was used to evaluate the model, calculating precision, recall, and F1-score. Fine-tuning was performed using the Trainer API from the Hugging Face library.\n",
    "\n",
    "### Tokenization and Alignment:\n",
    "The Hindi sentences were tokenized using the XLM-RoBERTa tokenizer, which produces subword tokens. The labels were aligned with these subword tokens by propagating chunk labels over subword tokens using the following strategy:\n",
    "- The first subword token of each word retains the chunk label.\n",
    "- Subsequent subword tokens are ignored unless they belong to an \"I-\" chunk (e.g., I-NP for inside a noun phrase).\n",
    "\n",
    "### Label Mapping:\n",
    "The chunk labels were encoded as integers, and a label-to-ID mapping was created to convert BIO labels to numeric form. This was crucial for training the token classification model.\n",
    "\n",
    "## 4. Results and Analysis\n",
    "The performance of the fine-tuned model was evaluated on both the **development** and **test** sets. Below are the key results from the experiments:\n",
    "\n",
    "### Validation Set Results:\n",
    "| Metric      | Value    |\n",
    "|-------------|----------|\n",
    "| Precision   | 98.652%   |\n",
    "| Recall      | 98.656%   |\n",
    "| F1-score    | 98.653%   |\n",
    "\n",
    "### Test Set Results:\n",
    "| Metric      | Value    |\n",
    "|-------------|----------|\n",
    "| Precision   | 98.515%   |\n",
    "| Recall      | 98.516%   |\n",
    "| F1-score    | 98.515%   |\n",
    "\n",
    "The model performed exceptionally well in chunking Hindi text, achieving an overall F1-score of **98.51%** on the test set. This demonstrates that XLM-RoBERTa is effective for syntactic chunking in a low-resource language like Hindi. \n",
    "\n",
    "The classification report shows that the model performed particularly well on noun phrases (NP) and verb phrases (VP), while the performance was slightly lower for smaller categories such as prepositional phrases (PP) and adjective phrases (ADJP).\n",
    "\n",
    "### Error Analysis:\n",
    "Some of the errors in chunking occurred due to:\n",
    "1. **Ambiguity in short sentences**: The model sometimes misclassified short, ambiguous sentences where the POS context was insufficient.\n",
    "2. **Subword tokenization errors**: Subword tokenization occasionally led to incorrect alignment between tokens and chunk labels, particularly for inflected forms or compound words in Hindi.\n",
    "\n",
    "## 5. Conclusion and Future Work\n",
    "This project successfully fine-tuned a pre-trained transformer model for the task of syntactic chunking in Hindi. The use of XLM-RoBERTa showed promising results, achieving high precision, recall, and F1 scores. The model demonstrated robustness in recognizing larger phrase chunks, such as noun and verb phrases.\n",
    "\n",
    "<!-- In future work, we could explore:\n",
    "1. **Subword alignment improvements**: Improving the handling of subword tokens to reduce misalignment errors.\n",
    "2. **Data augmentation**: Introducing additional training data to cover more linguistic phenomena in Hindi.\n",
    "3. **Cross-lingual transfer**: Applying the model to other low-resource languages using transfer learning techniques. -->\n",
    "\n",
    "### References:\n",
    "1. Universal Dependencies Hindi Treebank (HDTB).\n",
    "2. XLM-RoBERTa: Conneau, A., et al. \"Unsupervised cross-lingual representation learning at scale.\" (2020). \n",
    "\n",
    "This report demonstrates the potential of transformer models for syntactic chunking in low-resource languages, highlighting the flexibility and power of pre-trained multilingual models like XLM-RoBERTa for NLP tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
